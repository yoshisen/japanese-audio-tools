{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d764b9",
   "metadata": {},
   "source": [
    "# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SRTå­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆMP3ãªã©ï¼‰ã‚’æ—¥æœ¬èªéŸ³å£°èªè­˜ã—ã€ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä»˜ãã®SRTå­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚Whisper AIã‚’ä½¿ç”¨ã—ã¦é«˜ç²¾åº¦ãªæ–‡å­—èµ·ã“ã—ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "**ä¸»ãªæ©Ÿèƒ½:**\n",
    "- åŸºæœ¬çš„ãªéŸ³å£°èªè­˜ã¨SRTç”Ÿæˆ\n",
    "- é•·æ™‚é–“éŸ³å£°ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²å‡¦ç†\n",
    "- å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffa5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai-whisper \n",
    "!pip install numpy\n",
    "!pip install pydub\n",
    "!sudo apt-get install -y ffmpeg\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f317e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã®å ´åˆã¯éŸ³å£°èªè­˜ã‚’è¡Œã„ã€ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä»˜ãã®SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ffmpegã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…é ˆã§ã™ã€‚\n",
    "#ä¸»ã«æ—¥æœ¬èªèƒ½åŠ›è©¦é¨“ã®éŸ³å£°ã§ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "#https://www.gyan.dev/ffmpeg/builds/\n",
    "\n",
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "\n",
    "AUDIO_PATH = \"/path/to/your/audio.mp3\"        # å…ƒã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    print(\"ğŸ” éŸ³å£°ã‚’èªè­˜ã—ã¦ã„ã¾ã™...\")\n",
    "    \n",
    "    # å¿…ãšGPUã‚’ä½¿ç”¨ã™ã‚‹\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    model = whisper.load_model(\"medium\", device=device)\n",
    "    \n",
    "    # éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹\n",
    "    result = model.transcribe(audio_path, language='ja')\n",
    "    \n",
    "    # ç¾åœ¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    output_path = os.path.join(script_dir, \"output.srt\")\n",
    "    \n",
    "    # SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, segment in enumerate(result[\"segments\"]):\n",
    "            start = segment[\"start\"]\n",
    "            end = segment[\"end\"]\n",
    "            text = segment[\"text\"].strip()\n",
    "            f.write(f\"{i+1}\\n\")\n",
    "            f.write(f\"{format_time(start)} --> {format_time(end)}\\n\")\n",
    "            f.write(f\"{text}\\n\\n\")\n",
    "    \n",
    "    print(f\"âœ… æ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    print(f\"ğŸ“ SRT ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å ´æ‰€: {output_path}\")\n",
    "    print(f\"ğŸ“ åˆè¨ˆ {len(result['segments'])} å€‹ã®å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    millis = int((secs - int(secs)) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{int(secs):02},{millis:03}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        transcribe_audio(AUDIO_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {AUDIO_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ–‡å­—èµ·ã“ã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "from pydub import AudioSegment\n",
    "from typing import List, Dict\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "AUDIO_PATH = \"/path/to/your/long_audio.mp3\"        # å…ƒã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "SEGMENT_DURATION = 5* 60  # 5åˆ†ï¼ˆ300ç§’ï¼‰\n",
    "OVERLAP_DURATION = 5        # 5ç§’ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã§é€”åˆ‡ã‚Œã‚’é˜²ã\n",
    "OUTPUT_DIR = None           # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚Noneãªã‚‰è‡ªå‹•ã§æ±ºå®š\n",
    "\n",
    "class SegmentedTranscriber:\n",
    "    def __init__(self, model_size=\"medium\", device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"ğŸ–¥ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}\")\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "        print(\"ğŸ“¦ Whisperãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...\")\n",
    "        self.model = whisper.load_model(model_size, device=self.device)\n",
    "        print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "\n",
    "    def split_audio(self, audio_path: str) -> List[Dict]:\n",
    "        \"\"\"éŸ³å£°ã‚’10åˆ†ã”ã¨ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²ã™ã‚‹\"\"\"\n",
    "        print(\"ğŸ”ª éŸ³å£°ã‚’åˆ†å‰²ã—ã¦ã„ã¾ã™...\")\n",
    "        \n",
    "        # éŸ³å£°ã‚’èª­ã¿è¾¼ã‚€\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "        total_duration = len(audio) / 1000  # ç§’å˜ä½ã«å¤‰æ›ã™ã‚‹\n",
    "        \n",
    "        print(f\"ğŸ“Š éŸ³å£°ã®ç·æ™‚é–“: {self.format_time(total_duration)}\")\n",
    "        \n",
    "        segments = []\n",
    "        segment_count = math.ceil(total_duration / SEGMENT_DURATION)\n",
    "        \n",
    "        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        for i in range(segment_count):\n",
    "            start_time = i * SEGMENT_DURATION\n",
    "            \n",
    "            # çµ‚äº†æ™‚åˆ»ã‚’è¨ˆç®—ã—ã€æœ€å¾Œã®åŒºé–“ä»¥å¤–ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’åŠ ãˆã‚‹\n",
    "            if i < segment_count - 1:\n",
    "                end_time = min(start_time + SEGMENT_DURATION + OVERLAP_DURATION, total_duration)\n",
    "            else:\n",
    "                end_time = total_duration\n",
    "            \n",
    "            # éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º\n",
    "            start_ms = int(start_time * 1000)\n",
    "            end_ms = int(end_time * 1000)\n",
    "            segment_audio = audio[start_ms:end_ms]\n",
    "            \n",
    "            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜\n",
    "            temp_file = os.path.join(temp_dir, f\"segment_{i:03d}.wav\")\n",
    "            segment_audio.export(temp_file, format=\"wav\")\n",
    "            \n",
    "            segments.append({\n",
    "                'file_path': temp_file,\n",
    "                'start_offset': start_time,\n",
    "                'end_offset': end_time,\n",
    "                'segment_index': i,\n",
    "                'actual_duration': (end_ms - start_ms) / 1000\n",
    "            })\n",
    "            \n",
    "            print(f\"ğŸ“‹ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {i+1}/{segment_count}: {self.format_time(start_time)} - {self.format_time(end_time)}\")\n",
    "        \n",
    "        return segments, temp_dir\n",
    "\n",
    "    def transcribe_segment(self, segment_info: Dict) -> List[Dict]:\n",
    "        \"\"\"å˜ä¸€ã®éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ–‡å­—èµ·ã“ã—\"\"\"\n",
    "        file_path = segment_info['file_path']\n",
    "        start_offset = segment_info['start_offset']\n",
    "        segment_index = segment_info['segment_index']\n",
    "        \n",
    "        print(f\"ğŸ¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {segment_index + 1} ã‚’æ–‡å­—èµ·ã“ã—ã—ã¦ã„ã¾ã™...\")\n",
    "        \n",
    "        # éŸ³å£°åŒºé–“ã‚’æ–‡å­—èµ·ã“ã—ã™ã‚‹\n",
    "        result = self.model.transcribe(file_path, language='ja')\n",
    "        \n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’èª¿æ•´\n",
    "        adjusted_segments = []\n",
    "        for seg in result[\"segments\"]:\n",
    "            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’åŠ ãˆã‚‹\n",
    "            adjusted_start = seg[\"start\"] + start_offset\n",
    "            adjusted_end = seg[\"end\"] + start_offset\n",
    "            \n",
    "            adjusted_segments.append({\n",
    "                'start': adjusted_start,\n",
    "                'end': adjusted_end,\n",
    "                'text': seg[\"text\"].strip(),\n",
    "                'segment_index': segment_index\n",
    "            })\n",
    "        \n",
    "        return adjusted_segments\n",
    "\n",
    "    def merge_overlapping_segments(self, all_segments: List[List[Dict]]) -> List[Dict]:\n",
    "        \"\"\"é‡è¤‡åŒºé–“ã‚’çµ±åˆã—ã¦é‡è¤‡å†…å®¹ã‚’å‰Šé™¤\"\"\"\n",
    "        print(\"ğŸ”— ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµåˆã—ã€é‡è¤‡éƒ¨åˆ†ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™...\")\n",
    "        \n",
    "        merged_segments = []\n",
    "        \n",
    "        for i, segments in enumerate(all_segments):\n",
    "            if i == 0:\n",
    "                # æœ€åˆã®åŒºé–“ã¯ãã®ã¾ã¾è¿½åŠ \n",
    "                merged_segments.extend(segments)\n",
    "            else:\n",
    "                # æ¬¡ä»¥é™ã®åŒºé–“ã¯é‡è¤‡ã‚’å‡¦ç†ã™ã‚‹\n",
    "                overlap_start = i * SEGMENT_DURATION\n",
    "                \n",
    "                # é‡è¤‡åŒºé–“ä»¥é™ã®éƒ¨åˆ†ã®ã¿è¿½åŠ \n",
    "                for seg in segments:\n",
    "                    if seg['start'] >= overlap_start:\n",
    "                        merged_segments.append(seg)\n",
    "        \n",
    "        # æ™‚é–“é †ã«ä¸¦ã³æ›¿ãˆã‚‹\n",
    "        merged_segments.sort(key=lambda x: x['start'])\n",
    "        \n",
    "        return merged_segments\n",
    "\n",
    "    def save_srt(self, segments: List[Dict], output_path: str):\n",
    "        \"\"\"SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹\"\"\"\n",
    "        print(\"ğŸ’¾ SRT ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™...\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for i, segment in enumerate(segments):\n",
    "                start = segment[\"start\"]\n",
    "                end = segment[\"end\"]\n",
    "                text = segment[\"text\"].strip()\n",
    "                \n",
    "                if text:  # ç©ºã§ãªã„ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã‚’ä¿å­˜\n",
    "                    f.write(f\"{i+1}\\n\")\n",
    "                    f.write(f\"{self.format_time(start)} --> {self.format_time(end)}\\n\")\n",
    "                    f.write(f\"{text}\\n\\n\")\n",
    "        \n",
    "        print(f\"âœ… SRT ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ: {output_path}\")\n",
    "        print(f\"ğŸ“ åˆè¨ˆ {len([s for s in segments if s['text'].strip()])} å€‹ã®å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ\")\n",
    "\n",
    "    def transcribe_audio(self, audio_path: str):\n",
    "        \"\"\"ãƒ¡ã‚¤ãƒ³ã®æ–‡å­—èµ·ã“ã—é–¢æ•°\"\"\"\n",
    "        print(\"ğŸš€ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†ã‘ã—ãŸæ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "        \n",
    "        # éŸ³å£°ã‚’åˆ†å‰²\n",
    "        segments_info, temp_dir = self.split_audio(audio_path)\n",
    "        \n",
    "        try:\n",
    "            all_segments = []\n",
    "            \n",
    "            # åŒºé–“ã”ã¨ã«æ–‡å­—èµ·ã“ã—\n",
    "            for segment_info in segments_info:\n",
    "                segment_result = self.transcribe_segment(segment_info)\n",
    "                all_segments.append(segment_result)\n",
    "                \n",
    "                # GPUãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # é‡è¤‡åŒºé–“ã‚’çµ±åˆã™ã‚‹\n",
    "            final_segments = self.merge_overlapping_segments(all_segments)\n",
    "            \n",
    "            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "            if OUTPUT_DIR:\n",
    "                script_dir = OUTPUT_DIR\n",
    "            else:\n",
    "                try:\n",
    "                    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å–å¾—ã‚’è©¦ã¿ã‚‹\n",
    "                    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "                except NameError:\n",
    "                    # Jupyter/IPython ã§ã¯ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨\n",
    "                    script_dir = os.getcwd()\n",
    "            \n",
    "            base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "            output_path = os.path.join(script_dir, f\"{base_name}_segmented.srt\")\n",
    "            \n",
    "            # SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹\n",
    "            self.save_srt(final_segments, output_path)\n",
    "            \n",
    "            # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º\n",
    "            total_duration = max(seg['end'] for seg in final_segments) if final_segments else 0\n",
    "            print(f\"\\nğŸ“Š æ–‡å­—èµ·ã“ã—çµ±è¨ˆ:\")\n",
    "            print(f\"   â€¢ ç·æ™‚é–“: {self.format_time(total_duration)}\")\n",
    "            print(f\"   â€¢ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments_info)}\")\n",
    "            print(f\"   â€¢ å­—å¹•æ•°: {len(final_segments)}\")\n",
    "            print(f\"   â€¢ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}\")\n",
    "            \n",
    "        finally:\n",
    "            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤\n",
    "            print(\"ğŸ§¹ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦ã„ã¾ã™...\")\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def format_time(seconds):\n",
    "        \"\"\"SRTå½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«æ•´å½¢\"\"\"\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = seconds % 60\n",
    "        millis = int((secs - int(secs)) * 1000)\n",
    "        return f\"{hours:02}:{minutes:02}:{int(secs):02},{millis:03}\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"ãƒ¡ã‚¤ãƒ³é–¢æ•°\"\"\"\n",
    "    try:\n",
    "        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ‰ç„¡ã‚’ç¢ºèª\n",
    "        if not os.path.exists(AUDIO_PATH):\n",
    "            print(f\"âŒ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {AUDIO_PATH}\")\n",
    "            return\n",
    "        \n",
    "        # ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèª\n",
    "        try:\n",
    "            from pydub import AudioSegment\n",
    "        except ImportError:\n",
    "            print(\"âŒ ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ pydub ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:\")\n",
    "            print(\"   pip install pydub\")\n",
    "            return\n",
    "        \n",
    "        # æ–‡å­—èµ·ã“ã—ç”¨ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ\n",
    "        transcriber = SegmentedTranscriber(model_size=\"medium\")\n",
    "        \n",
    "        # æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹\n",
    "        transcriber.transcribe_audio(AUDIO_PATH)\n",
    "        \n",
    "        print(\"\\nğŸ‰ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†ã‘ã—ãŸæ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ–‡å­—èµ·ã“ã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "AUDIO_PATH = \"/path/to/your/audio.mp3\"        # å…ƒã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "\n",
    "def transcribe_audio_optimized(audio_path):\n",
    "    print(\"ğŸ” éŸ³å£°ã‚’èªè­˜ã—ã¦ã„ã¾ã™...\")\n",
    "    \n",
    "    # å¿…ãšGPUã‚’ä½¿ç”¨ã™ã‚‹\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«èª­ã¿è¾¼ã¿ã€large-v2ã‚’æ¨å¥¨ï¼ˆæ™‚é–“è»¸ã®ç²¾åº¦ãŒå‘ä¸Šï¼‰\n",
    "    model = whisper.load_model(\"large-v2\", device=device)\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã—ã¦æ™‚é–“è»¸ã®ç²¾åº¦ã‚’é«˜ã‚ã‚‹\n",
    "    result = model.transcribe(\n",
    "        audio_path, \n",
    "        language='ja',\n",
    "        \n",
    "        # é‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼šå˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—\n",
    "        word_timestamps=True,\n",
    "        \n",
    "        # ãƒ‡ã‚³ãƒ¼ãƒ‰å“è³ªã‚’ä¸Šã’ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        beam_size=5,                    # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã§ç²¾åº¦ã‚’å‘ä¸Š\n",
    "        best_of=5,                      # è¤‡æ•°å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æœ€è‰¯çµæœã‚’æ¡ç”¨\n",
    "        temperature=0.0,                # æ±ºå®šçš„ãªå‡ºåŠ›ã§ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’é¿ã‘ã‚‹\n",
    "        \n",
    "        # æ–‡è„ˆã®ç´¯ç©èª¤å·®ã‚’é¿ã‘ã‚‹\n",
    "        condition_on_previous_text=False,\n",
    "        \n",
    "        # ç„¡éŸ³ã¨ä½å“è³ªã‚’æ¤œå‡º\n",
    "        no_speech_threshold=0.6,        # ç„¡éŸ³æ¤œå‡ºã®é–¾å€¤\n",
    "        logprob_threshold=-1.0,         # ä½ä¿¡é ¼åº¦ã®ãƒ•ã‚£ãƒ«ã‚¿\n",
    "        compression_ratio_threshold=2.4, # é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ•ã‚£ãƒ«ã‚¿\n",
    "        \n",
    "        # æ—¥æœ¬èªå‘ã‘ã®æœ€é©åŒ–\n",
    "        initial_prompt=\"ä»¥ä¸‹ã¯æ—¥æœ¬èªã®éŸ³å£°ã§ã™ã€‚\",\n",
    "        \n",
    "        # VADãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–\n",
    "        prepend_punctuations='\"\\'Â¿([{-',\n",
    "        append_punctuations='\"\\'.ã€‚,ï¼Œ!ï¼?ï¼Ÿ:ï¼š\")]}ã€'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… èªè­˜å®Œäº†ã€åˆè¨ˆ {len(result['segments'])} å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ\")\n",
    "    \n",
    "    # å˜èªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§åŒºé–“å¢ƒç•Œã‚’æœ€é©åŒ–\n",
    "    optimized_segments = optimize_segment_boundaries(result['segments'])\n",
    "    \n",
    "    # SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ\n",
    "    output_path = \"output_optimized.srt\"\n",
    "    save_srt_with_optimized_timing(optimized_segments, output_path)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def optimize_segment_boundaries(segments):\n",
    "    \"\"\"\n",
    "    å˜èªãƒ¬ãƒ™ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«åŸºã¥ã„ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å¢ƒç•Œã‚’æœ€é©åŒ–ã—ã€ä¸­é–“æ™‚é–“ã®åå·®ã‚’æ¸›ã‚‰ã™\n",
    "    \"\"\"\n",
    "    optimized_segments = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        # å˜èªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒã‚ã‚Œã°ã‚ˆã‚Šæ­£ç¢ºãªå¢ƒç•Œã‚’ä½¿ã†\n",
    "        if 'words' in segment and segment['words']:\n",
    "            words = segment['words']\n",
    "            \n",
    "            # æœ€åˆã¨æœ€å¾Œã®å˜èªã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å¢ƒç•Œã¨ã—ã¦ä½¿ã†\n",
    "            first_word_start = words[0]['start']\n",
    "            last_word_end = words[-1]['end']\n",
    "            \n",
    "            # æœ€é©åŒ–æ¸ˆã¿ã®åŒºé–“ã‚’ä½œæˆ\n",
    "            optimized_segment = {\n",
    "                'id': segment['id'],\n",
    "                'start': first_word_start,\n",
    "                'end': last_word_end,\n",
    "                'text': segment['text'],\n",
    "                'words': words\n",
    "            }\n",
    "            \n",
    "            # ç•°å¸¸ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ¤œæŸ»ã—ã¦ä¿®æ­£\n",
    "            if optimized_segment['start'] >= optimized_segment['end']:\n",
    "                optimized_segment['start'] = segment['start']\n",
    "                optimized_segment['end'] = segment['end']\n",
    "                \n",
    "        else:\n",
    "            # å˜èªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒç„¡ã„å ´åˆã¯å…ƒã®å€¤ã‚’åˆ©ç”¨\n",
    "            optimized_segment = segment\n",
    "            \n",
    "        optimized_segments.append(optimized_segment)\n",
    "    \n",
    "    # åŒºé–“ã®æ™‚é–“ãŒé‡ãªã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹\n",
    "    optimized_segments = fix_overlapping_segments(optimized_segments)\n",
    "    \n",
    "    return optimized_segments\n",
    "\n",
    "def fix_overlapping_segments(segments):\n",
    "    \"\"\"\n",
    "    é‡è¤‡ã™ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä¿®æ­£\n",
    "    \"\"\"\n",
    "    if len(segments) <= 1:\n",
    "        return segments\n",
    "    \n",
    "    fixed_segments = [segments[0]]\n",
    "    \n",
    "    for i in range(1, len(segments)):\n",
    "        current = segments[i].copy()\n",
    "        previous = fixed_segments[-1]\n",
    "        \n",
    "        # ç¾åœ¨ã®é–‹å§‹æ™‚åˆ»ãŒå‰ã®çµ‚äº†æ™‚åˆ»ã‚ˆã‚Šæ—©ã„å ´åˆ\n",
    "        if current['start'] < previous['end']:\n",
    "            # å¢ƒç•Œã‚’èª¿æ•´ï¼šä¸­é–“ç‚¹ã§åˆ†å‰²ã™ã‚‹\n",
    "            mid_point = (previous['end'] + current['start']) / 2\n",
    "            previous['end'] = mid_point\n",
    "            current['start'] = mid_point\n",
    "            \n",
    "            # å‰ã®åŒºé–“ã‚’æ›´æ–°\n",
    "            fixed_segments[-1] = previous\n",
    "        \n",
    "        fixed_segments.append(current)\n",
    "    \n",
    "    return fixed_segments\n",
    "\n",
    "def save_srt_with_optimized_timing(segments, output_path):\n",
    "    \"\"\"\n",
    "    æœ€é©åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, segment in enumerate(segments):\n",
    "            start = segment[\"start\"]\n",
    "            end = segment[\"end\"]\n",
    "            text = segment[\"text\"].strip()\n",
    "            \n",
    "            f.write(f\"{i+1}\\n\")\n",
    "            f.write(f\"{format_time(start)} --> {format_time(end)}\\n\")\n",
    "            f.write(f\"{text}\\n\\n\")\n",
    "    \n",
    "    print(f\"ğŸ“ æœ€é©åŒ–ã•ã‚ŒãŸSRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"\n",
    "    ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€ç²¾åº¦ã‚’å‘ä¸Š\n",
    "    \"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    millis = int((secs - int(secs)) * 1000)\n",
    "    return f\"{hours:02}:{minutes:02}:{int(secs):02},{millis:03}\"\n",
    "\n",
    "def process_long_audio_with_overlap(audio_path, segment_length=600, overlap=60):\n",
    "    \"\"\"\n",
    "    45-60åˆ†ã®é•·ã„éŸ³å£°ã«å¯¾ã™ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†ã§ã€ç´¯ç©èª¤å·®ã‚’æ¸›ã‚‰ã™\n",
    "    \"\"\"\n",
    "    print(\"ğŸµ é•·ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™...\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = whisper.load_model(\"large-v2\", device=device)\n",
    "    \n",
    "    # éŸ³å£°ã‚’èª­ã¿è¾¼ã‚“ã§ç·å†ç”Ÿæ™‚é–“ã‚’å–å¾—\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    total_duration = len(audio) / whisper.audio.SAMPLE_RATE\n",
    "    \n",
    "    print(f\"éŸ³å£°ã®ç·æ™‚é–“: {total_duration/60:.1f} åˆ†\")\n",
    "    \n",
    "    all_segments = []\n",
    "    current_time = 0\n",
    "    segment_id = 0\n",
    "    \n",
    "    while current_time < total_duration:\n",
    "        start_sample = int(current_time * whisper.audio.SAMPLE_RATE)\n",
    "        end_time = min(current_time + segment_length, total_duration)\n",
    "        end_sample = int(end_time * whisper.audio.SAMPLE_RATE)\n",
    "        \n",
    "        # éŸ³å£°ã‚¯ãƒªãƒƒãƒ—ã‚’æŠ½å‡º\n",
    "        audio_segment = audio[start_sample:end_sample]\n",
    "        \n",
    "        print(f\"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­: {current_time/60:.1f} - {end_time/60:.1f} åˆ†\")\n",
    "        \n",
    "        # åŒã˜æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åŒºé–“ã‚’æ–‡å­—èµ·ã“ã—\n",
    "        result = model.transcribe(\n",
    "            audio_segment,\n",
    "            language='ja',\n",
    "            word_timestamps=True,\n",
    "            beam_size=5,\n",
    "            best_of=5,\n",
    "            temperature=0.0,\n",
    "            condition_on_previous_text=False,\n",
    "            no_speech_threshold=0.6,\n",
    "            logprob_threshold=-1.0,\n",
    "            compression_ratio_threshold=2.4,\n",
    "            initial_prompt=\"ä»¥ä¸‹ã¯æ—¥æœ¬èªã®éŸ³å£°ã§ã™ã€‚\"\n",
    "        )\n",
    "        \n",
    "        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’èª¿æ•´ã—å…¨ä½“çµæœã«è¿½åŠ \n",
    "        for segment in result['segments']:\n",
    "            segment['start'] += current_time\n",
    "            segment['end'] += current_time\n",
    "            segment['id'] = segment_id\n",
    "            segment_id += 1\n",
    "            \n",
    "            # å˜èªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’èª¿æ•´\n",
    "            if 'words' in segment:\n",
    "                for word in segment['words']:\n",
    "                    word['start'] += current_time\n",
    "                    word['end'] += current_time\n",
    "            \n",
    "            all_segments.append(segment)\n",
    "        \n",
    "        current_time += segment_length - overlap\n",
    "    \n",
    "    # å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æœ€é©åŒ–\n",
    "    optimized_segments = optimize_segment_boundaries(all_segments)\n",
    "    \n",
    "    # çµæœã‚’ä¿å­˜\n",
    "    output_path = \"output_long_optimized.srt\"\n",
    "    save_srt_with_optimized_timing(optimized_segments, output_path)\n",
    "    \n",
    "    print(f\"âœ… é•·ã„éŸ³å£°ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€åˆè¨ˆ {len(optimized_segments)} å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ\")\n",
    "    return optimized_segments\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³é–¢æ•°\n",
    "def main():\n",
    "    print(\"ğŸŒ æ—¥æœ¬èª Whisper ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æœ€é©åŒ–æ–‡å­—èµ·ã“ã—\")\n",
    "    \n",
    "    # éŸ³å£°ã®é•·ã•ã‚’ç¢ºèªã—ã¦å‡¦ç†æ–¹å¼ã‚’æ±ºã‚ã‚‹\n",
    "    try:\n",
    "        audio = whisper.load_audio(AUDIO_PATH)\n",
    "        duration_minutes = len(audio) / whisper.audio.SAMPLE_RATE / 60\n",
    "        \n",
    "        print(f\"æ¤œå‡ºã•ã‚ŒãŸéŸ³å£°ã®é•·ã•: {duration_minutes:.1f} åˆ†\")\n",
    "        \n",
    "        if duration_minutes > 30:\n",
    "            print(\"é•·ã„éŸ³å£°æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™...\")\n",
    "            process_long_audio_with_overlap(AUDIO_PATH)\n",
    "        else:\n",
    "            print(\"æ¨™æº–æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™...\")\n",
    "            transcribe_audio_optimized(AUDIO_PATH)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "        print(\"æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ã‚’è©¦ã—ã¾ã™...\")\n",
    "        transcribe_audio_optimized(AUDIO_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
